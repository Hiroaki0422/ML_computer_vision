{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipdb\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/43/c3c2e866a8803e196d6209595020a4a6db1a3c5d07c01455669497ae23d0/ipdb-0.12.tar.gz\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from ipdb) (40.6.3)\n",
      "Requirement already satisfied: ipython>=5.1.0 in /anaconda3/lib/python3.6/site-packages (from ipdb) (7.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (4.3.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (4.6.0)\n",
      "Requirement already satisfied: backcall in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (0.13.2)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (0.1.0)\n",
      "Requirement already satisfied: pygments in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (2.3.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (2.0.7)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (4.3.0)\n",
      "Requirement already satisfied: pickleshare in /anaconda3/lib/python3.6/site-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
      "Requirement already satisfied: ipython_genutils in /anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (1.12.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /anaconda3/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /anaconda3/lib/python3.6/site-packages (from jedi>=0.10->ipython>=5.1.0->ipdb) (0.3.1)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.6/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.1.0->ipdb) (0.1.7)\n",
      "Building wheels for collected packages: ipdb\n",
      "  Running setup.py bdist_wheel for ipdb ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/hiroakioshima/Library/Caches/pip/wheels/59/24/91/695211bd228d40fb22dff0ce3f05ba41ab724ab771736233f3\n",
      "Successfully built ipdb\n",
      "Installing collected packages: ipdb\n",
      "Successfully installed ipdb-0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from mds189 import Mds189\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import ipdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for loading images.\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag for whether you're training or not\n",
    "is_train = True\n",
    "is_key_frame = True # TODO: set this to false to train on the video frames, instead of the key frames\n",
    "model_to_load = 'model.ckpt' # This is the model to load during testing, if you want to eval a previously-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for data loader\n",
    "params = {'batch_size': 32,  # TODO: fill in the batch size. often, these are things like 32,64,128,or 256\n",
    "          'shuffle': True,\n",
    "          'num_workers': 2 \n",
    "          }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.1\n",
    "# NOTE: depending on your optimizer, you may want to tune other hyperparameters as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "# TODO: put the path to your train, test, validation txt files\n",
    "if is_key_frame:\n",
    "    label_file_train =  'dataloader_files/keyframe_data_train.txt'\n",
    "    label_file_val  =  'dataloader_files/keyframe_data_val.txt'\n",
    "    # NOTE: the kaggle competition test data is only for the video frames, not the key frames\n",
    "    # this is why we don't have an equivalent label_file_test with keyframes\n",
    "else:\n",
    "    label_file_train = '../data/hw6_mds189/videoframe_data_train.txt'\n",
    "    label_file_val = '../data/hw6_mds189/videoframe_data_val.txt'\n",
    "    label_file_test = '../data/hw6_mds189/videoframe_data_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you should normalize based on the average image in the training set. This shows \n",
    "# an example of doing normalization\n",
    "mean = [134.010302198/255.0, 118.599587912/255.0, 102.038804945/255.0]\n",
    "std = [23.5033438916/255.0, 23.8827343458/255.0, 24.5498666589/255.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding & resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: if you want to pad or resize your images, you can put the parameters for that below.\n",
    "# transforms.Pad(padding=0, fill=0), # TODO: if you want to pad your images\n",
    "# Generators\n",
    "# NOTE: if you don't want to pad or resize your images, you should delete the Pad and Resize\n",
    "# transforms from all three _dataset definitions.\n",
    "train_dataset = Mds189(label_file_train,loader=default_loader,transform=transforms.Compose([\n",
    "                                               transforms.Resize([227,227]), # TODO: if you want to resize your images\n",
    "                                               transforms.Grayscale(3), \n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "train_loader = data.DataLoader(train_dataset, **params)\n",
    "\n",
    "val_dataset = Mds189(label_file_val,loader=default_loader,transform=transforms.Compose([\n",
    "                                               transforms.Resize([227, 227]),\n",
    "                                               transforms.Grayscale(3), \n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "val_loader = data.DataLoader(val_dataset, **params)\n",
    "\n",
    "if not is_key_frame:\n",
    "    test_dataset = Mds189(label_file_test,loader=default_loader,transform=transforms.Compose([\n",
    "                                                   transforms.Resize([227, 227]),\n",
    "                                                   transforms.Grayscale(3), \n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean, std)\n",
    "                                               ]))\n",
    "    test_loader = data.DataLoader(test_dataset, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one way of defining your model architecture is to fill in a class like NeuralNet()\n",
    "# NOTE: you should not overwrite the models you try whose performance you're keeping track of.\n",
    "#       one thing you could do is have many different model forward passes in class NeuralNet()\n",
    "#       and then depending on which model you want to train/evaluate, you call that model's\n",
    "#       forward pass. this strategy will save you a lot of time in the long run. the last thing\n",
    "#       you want to do is have to recode the layer structure for a model (whose performance\n",
    "#       you're reporting) because you forgot to e.g., compute the confusion matrix on its results\n",
    "#       or visualize the error modes of your (best) model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # you can define some common layers, for example: \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 8),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # now you can use the layers you defined, to write the forward pass, i.e.,\n",
    "        # network architecture for your model\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we're only testing, we don't want to train for any epochs, and we want to load a model\n",
    "if not is_train:\n",
    "    num_epochs = 0\n",
    "    model.load_state_dict(torch.load('model.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion =  torch.nn.CrossEntropyLoss().cuda() #TODO: define your loss here. hint: should just require calling a built-in pytorch layer.\n",
    "# NOTE: you can use a different optimizer besides Adam, like RMSProp or SGD, if you'd like\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 3, 3, 5, 1, 6, 5, 7, 4, 6, 6, 4, 7, 3, 6, 3, 4, 5, 6, 4, 6, 5, 4,\n",
      "        5, 4, 4, 2, 7, 6, 6, 5])\n",
      "tensor([3, 2, 2, 4, 5, 1, 5, 6, 1, 6, 0, 6, 4, 4, 6, 6, 0, 4, 3, 3, 4, 6, 2, 3,\n",
      "        2, 5, 5, 4, 5, 6, 1, 6])\n",
      "tensor([4, 3, 6, 1, 3, 2, 7, 1, 6, 7, 4, 3, 4, 5, 0, 5, 1, 5, 2, 6, 5, 3, 7, 2,\n",
      "        7, 6, 6, 6, 6, 7, 5, 3])\n",
      "tensor([6, 3, 5, 2, 6, 3, 3, 3, 4, 5, 6, 6, 2, 5, 1, 2, 2, 5, 6, 3, 4, 6, 3, 4,\n",
      "        4, 6, 1, 5, 3, 2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (local_batch,local_labels) in enumerate(train_loader):\n",
    "    print(local_labels)\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Loop over epochs\n",
    "def train_model(model, optimizer, criterion, learning_rate, num_epochs, decay_rate=1, decay_start=3):\n",
    "    print('Beginning training..')\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        if epoch >= decay_start:\n",
    "            learning_rate = learning_rate * decay_rate\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        print('epoch {}'.format(epoch))\n",
    "        for i, (local_batch,local_labels) in enumerate(train_loader):\n",
    "            # Transfer to GPU\n",
    "            local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.forward(local_ims)\n",
    "            loss = criterion(outputs, local_labels)\n",
    "            # TODO: maintain a list of your losses as a function of number of steps\n",
    "            #       because we ask you to plot this information\n",
    "            # NOTE: if you use Google Colab's tensorboard-like feature to visualize\n",
    "            #       the loss, you do not need to plot it here. just take a screenshot\n",
    "            #       of the loss curve and include it in your write-up.\n",
    "            # loss_list =\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 4 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    end = time.time()\n",
    "    print('Time: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training..\n",
      "epoch 0\n",
      "Epoch [1/8], Step [4/91], Loss: 2.0747\n",
      "Epoch [1/8], Step [8/91], Loss: 2.0569\n",
      "Epoch [1/8], Step [12/91], Loss: 2.0406\n",
      "Epoch [1/8], Step [16/91], Loss: 2.0605\n",
      "Epoch [1/8], Step [20/91], Loss: 2.1084\n",
      "Epoch [1/8], Step [24/91], Loss: 2.0260\n",
      "Epoch [1/8], Step [28/91], Loss: 2.0544\n",
      "Epoch [1/8], Step [32/91], Loss: 1.9621\n",
      "Epoch [1/8], Step [36/91], Loss: 2.0264\n",
      "Epoch [1/8], Step [40/91], Loss: 2.0100\n",
      "Epoch [1/8], Step [44/91], Loss: 1.9961\n",
      "Epoch [1/8], Step [48/91], Loss: 2.0442\n",
      "Epoch [1/8], Step [52/91], Loss: 1.9665\n",
      "Epoch [1/8], Step [56/91], Loss: 1.9821\n",
      "Epoch [1/8], Step [60/91], Loss: 1.9738\n",
      "Epoch [1/8], Step [64/91], Loss: 2.0176\n",
      "Epoch [1/8], Step [68/91], Loss: 2.0223\n",
      "Epoch [1/8], Step [72/91], Loss: 1.9585\n",
      "Epoch [1/8], Step [76/91], Loss: 2.0022\n",
      "Epoch [1/8], Step [80/91], Loss: 2.0974\n",
      "Epoch [1/8], Step [84/91], Loss: 1.9828\n",
      "Epoch [1/8], Step [88/91], Loss: 1.9703\n",
      "epoch 1\n",
      "Epoch [2/8], Step [4/91], Loss: 1.8846\n",
      "Epoch [2/8], Step [8/91], Loss: 2.0777\n",
      "Epoch [2/8], Step [12/91], Loss: 2.0756\n",
      "Epoch [2/8], Step [16/91], Loss: 2.0592\n",
      "Epoch [2/8], Step [20/91], Loss: 1.9080\n",
      "Epoch [2/8], Step [24/91], Loss: 1.9784\n",
      "Epoch [2/8], Step [28/91], Loss: 1.9831\n",
      "Epoch [2/8], Step [32/91], Loss: 1.8869\n",
      "Epoch [2/8], Step [36/91], Loss: 1.9198\n",
      "Epoch [2/8], Step [40/91], Loss: 1.7587\n",
      "Epoch [2/8], Step [44/91], Loss: 2.0662\n",
      "Epoch [2/8], Step [48/91], Loss: 1.9947\n",
      "Epoch [2/8], Step [52/91], Loss: 2.0836\n",
      "Epoch [2/8], Step [56/91], Loss: 1.7180\n",
      "Epoch [2/8], Step [60/91], Loss: 1.9289\n",
      "Epoch [2/8], Step [64/91], Loss: 2.0884\n",
      "Epoch [2/8], Step [68/91], Loss: 2.0386\n",
      "Epoch [2/8], Step [72/91], Loss: 1.9555\n",
      "Epoch [2/8], Step [76/91], Loss: 1.9747\n",
      "Epoch [2/8], Step [80/91], Loss: 1.8105\n",
      "Epoch [2/8], Step [84/91], Loss: 1.7331\n",
      "Epoch [2/8], Step [88/91], Loss: 1.9204\n",
      "epoch 2\n",
      "Epoch [3/8], Step [4/91], Loss: 1.9050\n",
      "Epoch [3/8], Step [8/91], Loss: 1.8119\n",
      "Epoch [3/8], Step [12/91], Loss: 1.4047\n",
      "Epoch [3/8], Step [16/91], Loss: 1.4319\n",
      "Epoch [3/8], Step [20/91], Loss: 1.6912\n",
      "Epoch [3/8], Step [24/91], Loss: 1.4752\n",
      "Epoch [3/8], Step [28/91], Loss: 1.6346\n",
      "Epoch [3/8], Step [32/91], Loss: 1.7914\n",
      "Epoch [3/8], Step [36/91], Loss: 1.7035\n",
      "Epoch [3/8], Step [40/91], Loss: 1.2130\n",
      "Epoch [3/8], Step [44/91], Loss: 1.8072\n",
      "Epoch [3/8], Step [48/91], Loss: 1.6973\n",
      "Epoch [3/8], Step [52/91], Loss: 1.7227\n",
      "Epoch [3/8], Step [56/91], Loss: 1.5645\n",
      "Epoch [3/8], Step [60/91], Loss: 1.1096\n",
      "Epoch [3/8], Step [64/91], Loss: 1.3406\n",
      "Epoch [3/8], Step [68/91], Loss: 0.9951\n",
      "Epoch [3/8], Step [72/91], Loss: 2.0055\n",
      "Epoch [3/8], Step [76/91], Loss: 1.5327\n",
      "Epoch [3/8], Step [80/91], Loss: 1.5078\n",
      "Epoch [3/8], Step [84/91], Loss: 1.5453\n",
      "Epoch [3/8], Step [88/91], Loss: 1.4128\n",
      "epoch 3\n",
      "Epoch [4/8], Step [4/91], Loss: 1.1200\n",
      "Epoch [4/8], Step [8/91], Loss: 0.7696\n",
      "Epoch [4/8], Step [12/91], Loss: 1.3746\n",
      "Epoch [4/8], Step [16/91], Loss: 1.1186\n",
      "Epoch [4/8], Step [20/91], Loss: 1.2672\n",
      "Epoch [4/8], Step [24/91], Loss: 1.1359\n",
      "Epoch [4/8], Step [28/91], Loss: 1.0196\n",
      "Epoch [4/8], Step [32/91], Loss: 1.0717\n",
      "Epoch [4/8], Step [36/91], Loss: 0.7186\n",
      "Epoch [4/8], Step [40/91], Loss: 1.2144\n",
      "Epoch [4/8], Step [44/91], Loss: 1.1870\n",
      "Epoch [4/8], Step [48/91], Loss: 1.1146\n",
      "Epoch [4/8], Step [52/91], Loss: 1.5342\n",
      "Epoch [4/8], Step [56/91], Loss: 1.1361\n",
      "Epoch [4/8], Step [60/91], Loss: 0.9380\n",
      "Epoch [4/8], Step [64/91], Loss: 1.1715\n",
      "Epoch [4/8], Step [68/91], Loss: 1.3407\n",
      "Epoch [4/8], Step [72/91], Loss: 1.1752\n",
      "Epoch [4/8], Step [76/91], Loss: 1.3553\n",
      "Epoch [4/8], Step [80/91], Loss: 1.3052\n",
      "Epoch [4/8], Step [84/91], Loss: 1.5334\n",
      "Epoch [4/8], Step [88/91], Loss: 0.6624\n",
      "epoch 4\n",
      "Epoch [5/8], Step [4/91], Loss: 1.0046\n",
      "Epoch [5/8], Step [8/91], Loss: 0.9840\n",
      "Epoch [5/8], Step [12/91], Loss: 1.3006\n",
      "Epoch [5/8], Step [16/91], Loss: 1.1387\n",
      "Epoch [5/8], Step [20/91], Loss: 0.9153\n",
      "Epoch [5/8], Step [24/91], Loss: 0.9706\n",
      "Epoch [5/8], Step [28/91], Loss: 1.0736\n",
      "Epoch [5/8], Step [32/91], Loss: 0.8140\n",
      "Epoch [5/8], Step [36/91], Loss: 0.9375\n",
      "Epoch [5/8], Step [40/91], Loss: 0.6815\n",
      "Epoch [5/8], Step [44/91], Loss: 0.8239\n",
      "Epoch [5/8], Step [48/91], Loss: 1.0436\n",
      "Epoch [5/8], Step [52/91], Loss: 0.5464\n",
      "Epoch [5/8], Step [56/91], Loss: 0.5471\n",
      "Epoch [5/8], Step [60/91], Loss: 0.9103\n",
      "Epoch [5/8], Step [64/91], Loss: 0.5789\n",
      "Epoch [5/8], Step [68/91], Loss: 0.7997\n",
      "Epoch [5/8], Step [72/91], Loss: 0.7789\n",
      "Epoch [5/8], Step [76/91], Loss: 0.7614\n",
      "Epoch [5/8], Step [80/91], Loss: 1.1545\n",
      "Epoch [5/8], Step [84/91], Loss: 0.6949\n",
      "Epoch [5/8], Step [88/91], Loss: 0.9658\n",
      "epoch 5\n",
      "Epoch [6/8], Step [4/91], Loss: 1.1125\n",
      "Epoch [6/8], Step [8/91], Loss: 0.7348\n",
      "Epoch [6/8], Step [12/91], Loss: 0.8039\n",
      "Epoch [6/8], Step [16/91], Loss: 0.6919\n",
      "Epoch [6/8], Step [20/91], Loss: 0.5865\n",
      "Epoch [6/8], Step [24/91], Loss: 0.6474\n",
      "Epoch [6/8], Step [28/91], Loss: 0.8533\n",
      "Epoch [6/8], Step [32/91], Loss: 0.8635\n",
      "Epoch [6/8], Step [36/91], Loss: 0.6669\n",
      "Epoch [6/8], Step [40/91], Loss: 0.9311\n",
      "Epoch [6/8], Step [44/91], Loss: 0.5437\n",
      "Epoch [6/8], Step [48/91], Loss: 0.7897\n",
      "Epoch [6/8], Step [52/91], Loss: 0.6093\n",
      "Epoch [6/8], Step [56/91], Loss: 0.8291\n",
      "Epoch [6/8], Step [60/91], Loss: 0.6198\n",
      "Epoch [6/8], Step [64/91], Loss: 0.7256\n",
      "Epoch [6/8], Step [68/91], Loss: 0.7599\n",
      "Epoch [6/8], Step [72/91], Loss: 0.6685\n",
      "Epoch [6/8], Step [76/91], Loss: 0.7518\n",
      "Epoch [6/8], Step [80/91], Loss: 0.6604\n",
      "Epoch [6/8], Step [84/91], Loss: 0.8429\n",
      "Epoch [6/8], Step [88/91], Loss: 0.6294\n",
      "epoch 6\n",
      "Epoch [7/8], Step [4/91], Loss: 0.8181\n",
      "Epoch [7/8], Step [8/91], Loss: 0.5530\n",
      "Epoch [7/8], Step [12/91], Loss: 0.4943\n",
      "Epoch [7/8], Step [16/91], Loss: 0.4581\n",
      "Epoch [7/8], Step [20/91], Loss: 0.3508\n",
      "Epoch [7/8], Step [24/91], Loss: 0.4559\n",
      "Epoch [7/8], Step [28/91], Loss: 0.7612\n",
      "Epoch [7/8], Step [32/91], Loss: 0.6201\n",
      "Epoch [7/8], Step [36/91], Loss: 0.8385\n",
      "Epoch [7/8], Step [40/91], Loss: 0.6788\n",
      "Epoch [7/8], Step [44/91], Loss: 0.7813\n",
      "Epoch [7/8], Step [48/91], Loss: 0.5036\n",
      "Epoch [7/8], Step [52/91], Loss: 0.4762\n",
      "Epoch [7/8], Step [56/91], Loss: 0.3748\n",
      "Epoch [7/8], Step [60/91], Loss: 0.7546\n",
      "Epoch [7/8], Step [64/91], Loss: 0.8064\n",
      "Epoch [7/8], Step [68/91], Loss: 0.6696\n",
      "Epoch [7/8], Step [72/91], Loss: 0.5988\n",
      "Epoch [7/8], Step [76/91], Loss: 0.5905\n",
      "Epoch [7/8], Step [80/91], Loss: 0.4659\n",
      "Epoch [7/8], Step [84/91], Loss: 0.2784\n",
      "Epoch [7/8], Step [88/91], Loss: 0.3775\n",
      "epoch 7\n",
      "Epoch [8/8], Step [4/91], Loss: 0.3729\n",
      "Epoch [8/8], Step [8/91], Loss: 0.5010\n",
      "Epoch [8/8], Step [12/91], Loss: 0.7518\n",
      "Epoch [8/8], Step [16/91], Loss: 0.4724\n",
      "Epoch [8/8], Step [20/91], Loss: 0.7405\n",
      "Epoch [8/8], Step [24/91], Loss: 0.6147\n",
      "Epoch [8/8], Step [28/91], Loss: 0.6352\n",
      "Epoch [8/8], Step [32/91], Loss: 0.4062\n",
      "Epoch [8/8], Step [36/91], Loss: 0.3595\n",
      "Epoch [8/8], Step [40/91], Loss: 0.5577\n",
      "Epoch [8/8], Step [44/91], Loss: 0.7566\n",
      "Epoch [8/8], Step [48/91], Loss: 0.3890\n",
      "Epoch [8/8], Step [52/91], Loss: 0.8398\n",
      "Epoch [8/8], Step [56/91], Loss: 0.4346\n",
      "Epoch [8/8], Step [60/91], Loss: 0.7569\n",
      "Epoch [8/8], Step [64/91], Loss: 0.4352\n",
      "Epoch [8/8], Step [68/91], Loss: 0.4540\n",
      "Epoch [8/8], Step [72/91], Loss: 0.8610\n",
      "Epoch [8/8], Step [76/91], Loss: 0.4111\n",
      "Epoch [8/8], Step [80/91], Loss: 0.6137\n",
      "Epoch [8/8], Step [84/91], Loss: 0.3680\n",
      "Epoch [8/8], Step [88/91], Loss: 0.5373\n",
      "Time: 2885.9049711227417\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train_model(model, optimizer, criterion, 0.1, 8, decay_rate=0.5, decay_start=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "output_list = []\n",
    "labels_list = []\n",
    "for i, (val_batch,val_labels) in enumerate(val_loader):\n",
    "    val_ims, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "    outputs = model.forward(val_ims)\n",
    "    labels = outputs.argmax(1)\n",
    "    output_list.append(labels)\n",
    "    labels_list.append(val_labels)\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7302564102564103"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "output_labels = torch.cat(output_list)\n",
    "val_labels = torch.cat(labels_list)\n",
    "accuracy_score(output_labels, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best so far :\n",
    "train_model(model, optimizer, criterion, 0.1, 8, decay_rate=0.5) with decay >=3\n",
    "with gray scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation & accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Testing..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-29b54e9cf8c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpredicted_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgroundtruth_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocal_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Transfer to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlocal_ims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Beginning Testing..')\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    for (local_batch,local_labels) in test_loader:\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        outputs = model.forward(local_ims)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += local_labels.size(0)\n",
    "        predicted_list.extend(predicted)\n",
    "        groundtruth_list.extend(local_labels)\n",
    "        correct += (predicted == local_labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "# Look at some things about the model results..\n",
    "# convert the predicted_list and groundtruth_list Tensors to lists\n",
    "pl = [p.cpu().numpy().tolist() for p in predicted_list]\n",
    "gt = [p.cpu().numpy().tolist() for p in groundtruth_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use pl and gt to produce your confusion matrices\n",
    "\n",
    "# view the per-movement accuracy\n",
    "label_map = ['reach','squat','inline','lunge','hamstrings','stretch','deadbug','pushup']\n",
    "for id in range(len(label_map)):\n",
    "    print('{}: {}'.format(label_map[id],sum([p and g for (p,g) in zip(np.array(pl)==np.array(gt),np.array(gt)==id)])/(sum(np.array(gt)==id)+0.)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you'll need to run the forward pass on the kaggle competition images, and save those results to a csv file.\n",
    "if not is_key_frame:\n",
    "    # your code goes here!\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
